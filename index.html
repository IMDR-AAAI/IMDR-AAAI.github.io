<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Incomplete Modality Disentangled Representation for Ophthalmic Disease Grading and Diagnosis</title>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Incomplete Modality Disentangled Representation for Ophthalmic Disease Grading and Diagnosis</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Chengzhi Liu*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Zile Huang*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Zhe Chen</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Feilong Tang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Yu Tian</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="">Zhongxing Xu</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="">Zihong Luo</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="">Yalin Zheng</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="">Yanda Meng†</a><sup>5</sup>
            </span>      
            
          </div>

          <div class="is-size-5 publication-authors">
             <span class="author-block"><sup>1</sup>University of Liverpool,</span>
            <span class="author-block"><sup>2</sup>Monash University,</span>
            <span class="author-block"><sup>3</sup>Cornell University,</span>
            <span class="author-block"><sup>4</sup>Harvard University,</span>
            <span class="author-block"><sup>5</sup>University of Exeter</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--               <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/FeilongTangmonash/IMDR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
<!--               <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div> -->

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="margin-top: -2.6rem;">
  <figure class="image is-centered" style="width: 60%; margin: 0 auto 1rem; text-align: center;">
    <img 
      src="./static/images/method_00.png" 
      alt="image" 
      style="width: 120%; height: auto; margin-left: 5%;" 
      loading="lazy"
    />
    <figcaption style="text-align: center;">
      Figure 1. (a) “Vanilla” denotes the latent subspace methods. (b) Our proposed IMDR strategy employs explicit constraints to minimize mutual information to the Disentangle Extraction layer, guided by a joint distribution, to effectively decouple multimodal data. (c) Illustration of the intra-modality inter-channel distance and inter-modality inter-channel distance between feature maps of the encoders from different modalities. The definition of channel distance is detailed in the Appendix B. “Single” is the model that trains the encoders of each modality independently, providing the ideal feature diversity without inter-modality interference. “:A” denotes the histogram mean. Lower inter-channel similarity means higher diversity.
    </figcaption>
  </figure>
</section>

  
 <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Ophthalmologists typically require multimodal data sources to improve diagnostic accuracy in clinical decisions. However, due to medical device shortages, low-quality data and data privacy concerns, missing data modalities are common in real-world scenarios. Existing deep learning methods tend to address it by learning an implicit latent subspace representation for different modality combinations. We identify two significant limitations of these methods: (1) implicit representation constraints that hinder the model's ability to capture modality-specific information and (2) modality heterogeneity, causing distribution gaps and redundancy in feature representations. To address these, we propose an Incomplete Modality Disentangled Representation (IMDR) strategy, which disentangles features into explicit independent modal-common and modal-specific features by guidance of mutual information, distilling informative knowledge and enabling it to reconstruct valuable missing semantics and produce robust multimodal representations. Furthermore, we introduce a joint proxy learning module that assists IMDR in eliminating intra-modality redundancy by exploiting the extracted proxies from each class. Experiments on four ophthalmology multimodal datasets demonstrate that the proposed IMDR outperforms the state-of-the-art methods significantly.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered" style="margin-top: 2rem;">
      <div class="column is-four-fifths">
      <h2 class="title is-3">Methodology</h2>
      </div>
      </div>

<!--    <section class="section" style="margin-top: -2.6rem;">
  <figure class="image is-centered" style="width: 80%; margin: 0 auto 1rem; text-align: center;">
    <img 
      src="./static/images/method.png" 
      alt="image" 
      style="width: 100%; height: auto;" 
      loading="lazy"
    />
    <figcaption style="text-align: center;">
      The scheme of the proposed FarSight strategy, which integrates with the softmax operation, replaces the traditional causal mask. Specifically, the attention score matrix ω is cleared of attention values in the upper triangular part, then register-attention scores are added using the matrix P, followed by the softmax computation. P has a linear decay in the upper triangular part and zeros in the lower triangular part. After the softmax operation, the remaining attention probabilities in the upper triangular part are cleared to ensure the causal decoding property is preserved.

    </figcaption>
  </figure>
</section> -->

  <section class="section" style="margin-top: -2.6rem;">
  <figure class="image is-centered" style="width: 130%; margin: 0 auto 1rem; text-align: center;">
    <img 
      src="./static/images/Model_network_00.png" 
      alt="image" 
      style="width: 150%; height: auto; margin-left: 5%" 
      loading="lazy"
    />
    <figcaption style="text-align: center;">
    <p>Figure 2. Overview of Our Proposed Framewor: (a) We train a teacher model using complete modality data, followed by co-training with a student model on incomplete inputs for knowledge distillation. The distillation is supervised by feature loss <span class="equation">L<sub>Feat</sub></span> and logit loss <span class="equation">L<sub>Logit</sub></span>.         During the training of the teacher model, the encoder outputs the single-modality features <span class="equation">e<sup>f</sup></span> and <span class="equation">e<sup>O</sup></span>. We build a set of proxies for a modality, with each set representing a class. Positive proxies are selected by a similarity matrix between <span class="equation">ê</span> and <span class="equation">e</span>. All proxies are optimized through the proxy loss <span class="equation">L<sub>Prox</sub></span>. Consequently, <span class="equation">ê<sup>f,+</sup></span> and <span class="equation">ê<sup>O,+</sup></span>, together with features <span class="equation">e<sup>f</sup></span> and <span class="equation">e<sup>O</sup></span>, are passed to the IMDR.
(b)Details for the IMDR strategy: We estimate the distributions of <span class="equation">ê<sup>f,+</sup></span> and <span class="equation">ê<sup>O,+</sup></span>, then combine them using Equation <span class="equation">P(̂e | x<sup>f</sup>, x<sup>O</sup>)</span> to obtain the joint distribution. The modality-shared feature <span class="equation">s</span> is sampled from this distribution. This feature <span class="equation">s</span> guides the decoupling via an attention layer, supervised by the loss <span class="equation">L<sub>MI</sub></span> to minimize the mutual information between the extracted shared features <span class="equation">̂s</span> and specific features <span class="equation">R<sup>f</sup></span>, <span class="equation">R<sup>O</sup></span>, as well as between <span class="equation">R<sup>f</sup></span> and <span class="equation">R<sup>O</sup></span>. </p>
    </figcaption>
  </figure>
</section>



   <div class="columns is-centered has-text-centered" style="margin-top: 0rem;">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Results</h2>
  </div>
</div>

<section class="section" style="margin-top: -2.6rem;">
  <figure class="image is-centered" style="width: 120%; margin: 0 auto 1rem; text-align: center;">
    <img 
      src="./static/images/AAAI2025_Retinal_OCT_00.png" 
      alt="image" 
      style="width: 100%; height: auto;" 
      loading="lazy"
    />
    <figcaption style="text-align: center;">
      Figure 3. Under Complete-Modality conditions, we evaluate our IMDR model against other models using two baseline architectures: CNN and Transformer. Specifically, we utilize 2D/3D-ResNet50  as CNN backbones, while Swin-Transformer and UNETR are employed as Transformer backbones. Under Missing-Modality conditions, all models uniformly use 2D/3D-ResNet-50 backbones and apply the same distillation method, as outlined in our approach, to ensure experimental consistency. The model’s efficacy is assessed using four key metrics: ACC, AUC, F1 and Spec. 
    </figcaption>
  </figure>
</section>

<section class="section" style="margin-top: -2.6rem;">
  <figure class="image is-centered" style="width: 120%; margin: 0 auto 1rem; text-align: center;">
    <img 
      src="./static/images/noise_00.png" 
      alt="image" 
      style="width: 120%; height: auto;" 
      loading="lazy"
    />
    <figcaption style="text-align: center;">
     Figure 4. The comparison of performance across various missing rates under intra-modality incompleteness.
    </figcaption>
  </figure>
</section>


   <div class="columns is-centered has-text-centered" style="margin-top: 0rem;">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Visualization</h2>
  </div>
</div>

   <section class="section" style="margin-top: -2.6rem;">
  <figure class="image is-centered" style="width: 120%; margin: 0 auto 1rem; text-align: center;">
    <img 
      src="./static/images/cam_4data_00.png" 
      alt="image" 
      style="width: 85%; height: auto; margin-left: 5%" 
      loading="lazy"
    />
    <figcaption style="text-align: center;">
      Figure 5. Comparative visualization of attention maps under
the inter-modality incompleteness setting: The first row corresponds to the Harvard-30k AMD dataset; the second row corresponds to the Harvard-30k Glaucoma dataset; the third row corresponds to the Harvard-30k DR dataset, and the fourth row corresponds to the GAMMA dataset. For each dataset, we select a representative disease stage.
    </figcaption>
  </figure>
</section>



<section class="section" id="BibTeX">
  <div class="container content" style="max-width: 800px; margin: 0 auto;">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      
    </code></pre>
  </div>
</section>

</body>
</html>
